"""
Context Recall Evaluator í´ë˜ìŠ¤ì˜ ê° í•¨ìˆ˜ ì‘ë™ì„ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” í…ŒìŠ¤íŠ¸ ì½”ë“œ

ì´ ëª¨ë“ˆì€ ì‹¤ì œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ë³´ë‹¤ëŠ” ê° í•¨ìˆ˜ì˜ ì…ë ¥ê³¼ ì¶œë ¥, ì²˜ë¦¬ ê³¼ì •ì„
ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•˜ì—¬ í´ë˜ìŠ¤ì˜ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
"""

import os
from typing import Dict, List

import typer
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.tree import Tree

from ragas_eval.context_recall_evaluator import ContextRecallEvaluator, load_test_data

# ì½˜ì†” ì¶œë ¥ì„ ìœ„í•œ Rich Console ê°ì²´ ìƒì„±
console = Console()

# Typer ì•± ìƒì„±
app = typer.Typer(
    name="visualize-context-recall-test",
    help="Context Recall Evaluator í•¨ìˆ˜ ì‹œê°í™” í…ŒìŠ¤íŠ¸ ë„êµ¬",
    add_completion=False,
)


def visualize_load_test_data_function(jsonl_path: str):
    """
    load_test_data í•¨ìˆ˜ì˜ ë™ì‘ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.

    JSONL íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ íŒŒì‹±í•˜ëŠ” ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
    """
    # ì„¹ì…˜ ì œëª© ì¶œë ¥
    console.print("\n" + "=" * 80, style="bold cyan")
    console.print(
        "ğŸ“‚ [bold cyan]í•¨ìˆ˜ í…ŒìŠ¤íŠ¸: load_test_data()[/bold cyan]", style="bold"
    )
    console.print("=" * 80 + "\n", style="bold cyan")

    # ì…ë ¥ íŒŒì¼ ê²½ë¡œ í‘œì‹œ
    console.print(f"[yellow]ì…ë ¥ íŒŒì¼ ê²½ë¡œ:[/yellow] {jsonl_path}")

    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(jsonl_path):
        console.print(f"[red]âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {jsonl_path}[/red]")
        return None

    try:
        # ì‹¤ì œ í•¨ìˆ˜ ì‹¤í–‰
        samples = load_test_data(jsonl_path)

        # ê²°ê³¼ í†µê³„ ì¶œë ¥
        console.print(f"\n[green]âœ“ ë¡œë“œëœ ìƒ˜í”Œ ê°œìˆ˜:[/green] {len(samples)}ê°œ\n")

        # ê° ìƒ˜í”Œì„ í…Œì´ë¸”ë¡œ ì‹œê°í™”
        for index, sample in enumerate(samples[:3]):  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
            table = Table(
                title=f"ìƒ˜í”Œ #{index + 1}",
                show_header=True,
                header_style="bold magenta",
            )
            table.add_column("í•„ë“œ", style="cyan", width=20)
            table.add_column("ê°’", style="green")

            # ê° í•„ë“œë¥¼ í…Œì´ë¸” í–‰ìœ¼ë¡œ ì¶”ê°€
            table.add_row("ID", str(sample.get("id", "N/A")))
            table.add_row("ì§ˆë¬¸", sample.get("question", "N/A")[:100] + "...")
            table.add_row("ì •ë‹µ", sample.get("ground_truth", "N/A")[:100] + "...")
            table.add_row("ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸", str(sample.get("images_list", [])[:5]))

            console.print(table)
            console.print()

        # ì „ì²´ ìƒ˜í”Œ ìˆ˜ê°€ 3ê°œë³´ë‹¤ ë§ìœ¼ë©´ ì•ˆë‚´ ë©”ì‹œì§€ ì¶œë ¥
        if len(samples) > 3:
            console.print(
                f"[dim]... ì™¸ {len(samples) - 3}ê°œì˜ ìƒ˜í”Œì´ ë” ìˆìŠµë‹ˆë‹¤.[/dim]\n"
            )

        return samples

    except Exception as error:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
        console.print(f"[red]âŒ ì—ëŸ¬ ë°œìƒ:[/red] {str(error)}")
        return None


def visualize_build_evaluator_llm_function(evaluator: ContextRecallEvaluator):
    """
    _build_evaluator_llm í•¨ìˆ˜ì˜ ë™ì‘ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.

    OpenAI LLMì„ ìƒì„±í•˜ëŠ” ê³¼ì •ê³¼ ì„¤ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
    """
    # ì„¹ì…˜ ì œëª© ì¶œë ¥
    console.print("\n" + "=" * 80, style="bold cyan")
    console.print(
        "ğŸ¤– [bold cyan]í•¨ìˆ˜ í…ŒìŠ¤íŠ¸: _build_evaluator_llm()[/bold cyan]", style="bold"
    )
    console.print("=" * 80 + "\n", style="bold cyan")

    # API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        console.print(
            f"[green]âœ“ OPENAI_API_KEY í™•ì¸ë¨:[/green] {api_key[:10]}...{api_key[-4:]}"
        )
    else:
        console.print("[red]âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.[/red]")
        return None

    try:
        # ëª¨ë¸ëª… ì§€ì •
        model_name = "gpt-4o"
        console.print(f"[yellow]ì‚¬ìš©í•  ëª¨ë¸:[/yellow] {model_name}\n")

        # LLM ë¹Œë“œ ì‹¤í–‰
        evaluator_llm = evaluator._build_evaluator_llm(model_name)

        # LLM ê°ì²´ ì •ë³´ ì¶œë ¥
        console.print("[green]âœ“ LLM ë¹Œë” ìƒì„± ì™„ë£Œ[/green]")
        console.print(f"[dim]LLM íƒ€ì…:[/dim] {type(evaluator_llm).__name__}")
        console.print(
            f"[dim]LLM í´ë˜ìŠ¤:[/dim] {evaluator_llm.__class__.__module__}.{evaluator_llm.__class__.__name__}\n"
        )

        return evaluator_llm

    except Exception as error:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
        console.print(f"[red]âŒ ì—ëŸ¬ ë°œìƒ:[/red] {str(error)}")
        return None


def visualize_search_contexts_function(
    evaluator: ContextRecallEvaluator, query: str, top_k: int = 5
):
    """
    _search_contexts í•¨ìˆ˜ì˜ ë™ì‘ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.

    ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
    """
    # ì„¹ì…˜ ì œëª© ì¶œë ¥
    console.print("\n" + "=" * 80, style="bold cyan")
    console.print(
        "ğŸ” [bold cyan]í•¨ìˆ˜ í…ŒìŠ¤íŠ¸: _search_contexts()[/bold cyan]", style="bold"
    )
    console.print("=" * 80 + "\n", style="bold cyan")

    # ì…ë ¥ íŒŒë¼ë¯¸í„° ì¶œë ¥
    console.print(f"[yellow]ê²€ìƒ‰ ì¿¼ë¦¬:[/yellow] {query}")
    console.print(f"[yellow]Top K:[/yellow] {top_k}\n")

    try:
        # ê²€ìƒ‰ ì‹¤í–‰
        search_results = evaluator._search_contexts(query, top_k)

        # ê²€ìƒ‰ ê²°ê³¼ í†µê³„
        console.print(f"[green]âœ“ ê²€ìƒ‰ ì™„ë£Œ:[/green] {len(search_results)}ê°œì˜ ê²°ê³¼\n")

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ì‹œê°í™”
        tree = Tree("ğŸ” ê²€ìƒ‰ ê²°ê³¼", guide_style="bold bright_blue")

        for index, result in enumerate(search_results[:5]):  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
            # ê° ê²°ê³¼ë¥¼ íŠ¸ë¦¬ ë…¸ë“œë¡œ ì¶”ê°€
            result_node = tree.add(f"[cyan]ê²°ê³¼ #{index + 1}[/cyan]")

            # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ê°€
            text_content = result.get("text", "N/A")
            if len(text_content) > 100:
                text_content = text_content[:100] + "..."
            result_node.add(f"[green]í…ìŠ¤íŠ¸:[/green] {text_content}")

            # ì ìˆ˜ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if "score" in result:
                result_node.add(f"[yellow]ìœ ì‚¬ë„ ì ìˆ˜:[/yellow] {result['score']:.4f}")

            # ì´ë¯¸ì§€ IDê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if "image_ids" in result and result["image_ids"]:
                result_node.add(
                    f"[magenta]ì´ë¯¸ì§€ ID:[/magenta] {result['image_ids'][:3]}"
                )

        console.print(tree)
        console.print()

        # ì „ì²´ ê²°ê³¼ ìˆ˜ê°€ 5ê°œë³´ë‹¤ ë§ìœ¼ë©´ ì•ˆë‚´ ë©”ì‹œì§€
        if len(search_results) > 5:
            console.print(
                f"[dim]... ì™¸ {len(search_results) - 5}ê°œì˜ ê²°ê³¼ê°€ ë” ìˆìŠµë‹ˆë‹¤.[/dim]\n"
            )

        return search_results

    except Exception as error:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
        console.print(f"[red]âŒ ì—ëŸ¬ ë°œìƒ:[/red] {str(error)}")
        return None


def visualize_extract_contexts_and_images_function(
    evaluator: ContextRecallEvaluator, search_results: List[Dict]
):
    """
    _extract_contexts_and_images í•¨ìˆ˜ì˜ ë™ì‘ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.

    ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ IDë¥¼ ì¶”ì¶œí•˜ëŠ” ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
    """
    # ì„¹ì…˜ ì œëª© ì¶œë ¥
    console.print("\n" + "=" * 80, style="bold cyan")
    console.print(
        "ğŸ“¤ [bold cyan]í•¨ìˆ˜ í…ŒìŠ¤íŠ¸: _extract_contexts_and_images()[/bold cyan]",
        style="bold",
    )
    console.print("=" * 80 + "\n", style="bold cyan")

    # ì…ë ¥ ë°ì´í„° í†µê³„ ì¶œë ¥
    console.print(f"[yellow]ì…ë ¥ ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜:[/yellow] {len(search_results)}ê°œ\n")

    try:
        # ì¶”ì¶œ ì‹¤í–‰
        contexts, image_ids = evaluator._extract_contexts_and_images(search_results)

        # ì¶”ì¶œ ê²°ê³¼ í†µê³„
        console.print("[green]âœ“ ì¶”ì¶œ ì™„ë£Œ[/green]")
        console.print(f"  - ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜: {len(contexts)}ê°œ")
        console.print(f"  - ì¤‘ë³µ ì œê±°ëœ ì´ë¯¸ì§€ ID ê°œìˆ˜: {len(image_ids)}ê°œ\n")

        # ì»¨í…ìŠ¤íŠ¸ ëª©ë¡ì„ íŒ¨ë„ë¡œ í‘œì‹œ
        contexts_panel_content = ""
        for index, context in enumerate(contexts[:3]):  # ì²˜ìŒ 3ê°œë§Œ
            preview = context[:80] + "..." if len(context) > 80 else context
            contexts_panel_content += f"[cyan]{index + 1}.[/cyan] {preview}\n"

        if len(contexts) > 3:
            contexts_panel_content += (
                f"\n[dim]... ì™¸ {len(contexts) - 3}ê°œì˜ ì»¨í…ìŠ¤íŠ¸ê°€ ë” ìˆìŠµë‹ˆë‹¤.[/dim]"
            )

        console.print(
            Panel(
                contexts_panel_content, title="ğŸ“ ì¶”ì¶œëœ ì»¨í…ìŠ¤íŠ¸", border_style="green"
            )
        )

        # ì´ë¯¸ì§€ ID ëª©ë¡ì„ íŒ¨ë„ë¡œ í‘œì‹œ
        image_ids_content = ", ".join(image_ids[:10])
        if len(image_ids) > 10:
            image_ids_content += f" ... (ì™¸ {len(image_ids) - 10}ê°œ)"

        console.print(
            Panel(image_ids_content, title="ğŸ–¼ï¸ ì¶”ì¶œëœ ì´ë¯¸ì§€ ID", border_style="magenta")
        )
        console.print()

        return contexts, image_ids

    except Exception as error:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
        console.print(f"[red]âŒ ì—ëŸ¬ ë°œìƒ:[/red] {str(error)}")
        return None, None


def visualize_compute_average_recall_function(
    evaluator: ContextRecallEvaluator, ragas_dataset: List[Dict]
):
    """
    _compute_average_recall í•¨ìˆ˜ì˜ ë™ì‘ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.

    RAGASë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ë¦¬ì½œì„ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
    """
    # ì„¹ì…˜ ì œëª© ì¶œë ¥
    console.print("\n" + "=" * 80, style="bold cyan")
    console.print(
        "ğŸ“Š [bold cyan]í•¨ìˆ˜ í…ŒìŠ¤íŠ¸: _compute_average_recall()[/bold cyan]", style="bold"
    )
    console.print("=" * 80 + "\n", style="bold cyan")

    # ì…ë ¥ ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥
    console.print(
        f"[yellow]í‰ê°€í•  ë°ì´í„°ì…‹ í¬ê¸°:[/yellow] {len(ragas_dataset)}ê°œ ìƒ˜í”Œ\n"
    )

    # ë°ì´í„°ì…‹ ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°
    if ragas_dataset:
        sample = ragas_dataset[0]
        console.print("[dim]ë°ì´í„°ì…‹ ìƒ˜í”Œ êµ¬ì¡°:[/dim]")
        console.print(f"  - user_input: {sample.get('user_input', 'N/A')[:60]}...")
        console.print(
            f"  - retrieved_contexts: {len(sample.get('retrieved_contexts', []))}ê°œ"
        )
        console.print(f"  - reference: {sample.get('reference', 'N/A')[:60]}...\n")

    try:
        # RAGAS í‰ê°€ ì‹¤í–‰ (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
        console.print(
            "[yellow]â³ RAGAS í‰ê°€ ì‹¤í–‰ ì¤‘...[/yellow] (ì´ ì‘ì—…ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"
        )

        average_recall = evaluator._compute_average_recall(ragas_dataset)

        # ê²°ê³¼ë¥¼ í° íŒ¨ë„ë¡œ í‘œì‹œ
        result_content = f"""
[bold green]í‰ê·  Context Recall:[/bold green] {average_recall:.4f}

[dim]Context Recall ì§€í‘œ ì„¤ëª…:[/dim]
- ê°’ ë²”ìœ„: 0.0 ~ 1.0
- ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (1.0ì´ ìµœê³ )
- ì˜ë¯¸: ì •ë‹µ(ground truth)ì— ìˆëŠ” ì •ë³´ê°€ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— 
  ì–¼ë§ˆë‚˜ ì˜ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ë¥¼ ì¸¡ì •
        """

        console.print(
            Panel(
                result_content.strip(), title="âœ… í‰ê°€ ê²°ê³¼", border_style="bold green"
            )
        )
        console.print()

        return average_recall

    except Exception as error:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
        console.print(f"[red]âŒ ì—ëŸ¬ ë°œìƒ:[/red] {str(error)}")
        import traceback

        console.print(f"[dim]{traceback.format_exc()}[/dim]")
        return None


def visualize_full_evaluation_pipeline(jsonl_path: str, top_k: int = 5):
    """
    ì „ì²´ í‰ê°€ íŒŒì´í”„ë¼ì¸ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.

    evaluate_context_recall_at_k í•¨ìˆ˜ì˜ ì „ì²´ ì‹¤í–‰ íë¦„ì„ ë‹¨ê³„ë³„ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.
    """
    # ë©”ì¸ ì œëª© ì¶œë ¥
    console.print("\n" + "=" * 80, style="bold yellow")
    console.print(
        "ğŸš€ [bold yellow]ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸: evaluate_context_recall_at_k()[/bold yellow]",
        style="bold",
    )
    console.print("=" * 80 + "\n", style="bold yellow")

    try:
        # Evaluator ì´ˆê¸°í™”
        console.print("[yellow]â³ ContextRecallEvaluator ì´ˆê¸°í™” ì¤‘...[/yellow]")
        evaluator = ContextRecallEvaluator()
        console.print("[green]âœ“ ì´ˆê¸°í™” ì™„ë£Œ[/green]\n")

        # í‰ê°€ ì‹¤í–‰
        console.print(f"[yellow]â³ Top-{top_k} í‰ê°€ ì‹¤í–‰ ì¤‘...[/yellow]")
        average_recall, details, ragas_dataset, all_image_ids = (
            evaluator.evaluate_context_recall_at_k(jsonl_path, top_k=top_k)
        )

        # ìµœì¢… ê²°ê³¼ë¥¼ í…Œì´ë¸”ë¡œ í‘œì‹œ
        result_table = Table(
            title="ğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼", show_header=True, header_style="bold cyan"
        )
        result_table.add_column("í•­ëª©", style="yellow", width=30)
        result_table.add_column("ê°’", style="green", width=40)

        # í…Œì´ë¸”ì— ê²°ê³¼ ì¶”ê°€
        result_table.add_row("í‰ê·  Context Recall@K", f"{average_recall:.4f}")
        result_table.add_row("K ê°’", str(details["k"]))
        result_table.add_row("í‰ê°€ ìƒ˜í”Œ ìˆ˜", str(details["num_samples"]))
        result_table.add_row("ìƒì„±ëœ RAGAS ë°ì´í„°ì…‹ í¬ê¸°", f"{len(ragas_dataset)}ê°œ")
        result_table.add_row("ì´ë¯¸ì§€ ID ë¦¬ìŠ¤íŠ¸ ê°œìˆ˜", f"{len(all_image_ids)}ê°œ")

        console.print(result_table)
        console.print()

        # ì„±ëŠ¥ í•´ì„ ì¶œë ¥
        console.print(
            Panel(
                _interpret_recall_score(average_recall),
                title="ğŸ’¡ ê²°ê³¼ í•´ì„",
                border_style="blue",
            )
        )
        console.print()

        return average_recall, details, ragas_dataset, all_image_ids

    except Exception as error:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
        console.print(f"[red]âŒ ì—ëŸ¬ ë°œìƒ:[/red] {str(error)}")
        import traceback

        console.print(f"[dim]{traceback.format_exc()}[/dim]")
        return None


def _interpret_recall_score(score: float) -> str:
    """
    ë¦¬ì½œ ì ìˆ˜ë¥¼ í•´ì„í•˜ì—¬ ì„¤ëª… ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    ì ìˆ˜ ë²”ìœ„ì— ë”°ë¼ ì„±ëŠ¥ ìˆ˜ì¤€ì„ í‰ê°€í•©ë‹ˆë‹¤.
    """
    if score >= 0.9:
        return "[bold green]ğŸ‰ ìš°ìˆ˜[/bold green]\nê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì •ë‹µì„ ë§¤ìš° ì˜ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤."
    elif score >= 0.7:
        return "[bold blue]ğŸ‘ ì–‘í˜¸[/bold blue]\nê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì •ë‹µì„ ì˜ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤."
    elif score >= 0.5:
        return "[bold yellow]âš ï¸ ë³´í†µ[/bold yellow]\nê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ì •ë‹µì´ ë¶€ë¶„ì ìœ¼ë¡œë§Œ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
    else:
        return "[bold red]âŒ ê°œì„  í•„ìš”[/bold red]\nê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ì •ë‹µì´ ì¶©ë¶„íˆ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."


def run_all_tests(jsonl_path: str):
    """
    ëª¨ë“  ì‹œê°í™” í…ŒìŠ¤íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.

    ê° í•¨ìˆ˜ì˜ ë™ì‘ì„ ë‹¨ê³„ë³„ë¡œ ì‹œê°í™”í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤.
    """
    # í”„ë¡œê·¸ë¨ ì‹œì‘ ë©”ì‹œì§€
    console.print("\n" + "ğŸ”¬" * 40, style="bold magenta")
    console.print(
        "[bold magenta]Context Recall Evaluator í•¨ìˆ˜ ì‹œê°í™” í…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨[/bold magenta]",
        justify="center",
    )
    console.print("ğŸ”¬" * 40 + "\n", style="bold magenta")

    # 1. load_test_data í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
    samples = visualize_load_test_data_function(jsonl_path)

    if not samples:
        console.print(
            "[red]âš ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ì–´ ë‹¤ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.[/red]"
        )
        return

    # ê°„ë‹¨í•œ ëª©ì—… ë°ì´í„°ë¡œ ê°œë³„ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
    console.print("\n[yellow]â”â”â” ê°œë³„ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹œì‘ â”â”â”[/yellow]\n")

    try:
        # Evaluator ì´ˆê¸°í™”
        console.print("[yellow]â³ ContextRecallEvaluator ì´ˆê¸°í™” ì¤‘...[/yellow]")
        evaluator = ContextRecallEvaluator()
        console.print("[green]âœ“ ì´ˆê¸°í™” ì™„ë£Œ[/green]\n")

        # 2. _build_evaluator_llm í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
        visualize_build_evaluator_llm_function(evaluator)

        # 3. _search_contexts í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
        test_query = samples[0]["question"] if samples else "What is machine learning?"
        search_results = visualize_search_contexts_function(
            evaluator, test_query, top_k=5
        )

        if search_results:
            # 4. _extract_contexts_and_images í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
            contexts, image_ids = visualize_extract_contexts_and_images_function(
                evaluator, search_results
            )

            # 5. _compute_average_recall í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ (ê°„ë‹¨í•œ ëª©ì—… ë°ì´í„° ì‚¬ìš©)
            if contexts:
                mock_ragas_dataset = [
                    {
                        "user_input": test_query,
                        "retrieved_contexts": contexts,
                        "reference": samples[0].get("ground_truth", "Test reference"),
                    }
                ]

                # ì£¼ì˜: ì´ í•¨ìˆ˜ëŠ” ì‹¤ì œ API í˜¸ì¶œì„ í•˜ë¯€ë¡œ ë¹„ìš©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
                console.print(
                    "[yellow]âš ï¸ _compute_average_recall í…ŒìŠ¤íŠ¸ëŠ” ì‹¤ì œ APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.[/yellow]"
                )
                user_input = typer.prompt("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)")

                if user_input.lower() == "y":
                    visualize_compute_average_recall_function(
                        evaluator, mock_ragas_dataset
                    )
                else:
                    console.print(
                        "[dim]_compute_average_recall í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.[/dim]\n"
                    )

        # 6. ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
        console.print("\n[yellow]â”â”â” ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘ â”â”â”[/yellow]\n")
        console.print(
            "[yellow]âš ï¸ ì´ í…ŒìŠ¤íŠ¸ëŠ” ì‹¤ì œ APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë¹„ìš©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.[/yellow]"
        )
        user_input = typer.prompt("ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)")

        if user_input.lower() == "y":
            visualize_full_evaluation_pipeline(jsonl_path, top_k=5)
        else:
            console.print("[dim]ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.[/dim]\n")

    except Exception as error:
        # ìµœìƒìœ„ ì—ëŸ¬ í•¸ë“¤ë§
        console.print(f"[red]âŒ ì¹˜ëª…ì  ì—ëŸ¬ ë°œìƒ:[/red] {str(error)}")
        import traceback

        console.print(f"[dim]{traceback.format_exc()}[/dim]")

    # í”„ë¡œê·¸ë¨ ì¢…ë£Œ ë©”ì‹œì§€
    console.print("\n" + "ğŸ¬" * 40, style="bold magenta")
    console.print("[bold magenta]í…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨ ì¢…ë£Œ[/bold magenta]", justify="center")
    console.print("ğŸ¬" * 40 + "\n", style="bold magenta")


# Typer ì»¤ë§¨ë“œ ì •ì˜


@app.command("all")
def cmd_all(
    jsonl: str = typer.Option(
        "/home/kun/Desktop/multimodal/data/test_samples.jsonl",
        "--jsonl",
        help="í…ŒìŠ¤íŠ¸ ë°ì´í„° JSONL íŒŒì¼ ê²½ë¡œ",
    ),
):
    """
    ëª¨ë“  í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.

    ì „ì²´ ì‹œê°í™” í…ŒìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì‹¤í–‰í•˜ì—¬ ê° í•¨ìˆ˜ì˜ ë™ì‘ì„ í™•ì¸í•©ë‹ˆë‹¤.
    """
    run_all_tests(jsonl)


@app.command("load-data")
def cmd_load_data(
    jsonl: str = typer.Option(
        ...,
        "--jsonl",
        help="ë¡œë“œí•  JSONL íŒŒì¼ ê²½ë¡œ",
    ),
):
    """
    load_test_data() í•¨ìˆ˜ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

    JSONL íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ì˜¤ëŠ” ê³¼ì •ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    console.print("\n[bold cyan]ğŸ“‚ ë°ì´í„° ë¡œë“œ í…ŒìŠ¤íŠ¸ ì‹œì‘[/bold cyan]\n")
    visualize_load_test_data_function(jsonl)


@app.command("build-llm")
def cmd_build_llm():
    """
    _build_evaluator_llm() í•¨ìˆ˜ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

    OpenAI LLM ê°ì²´ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì„ í™•ì¸í•©ë‹ˆë‹¤.
    """
    console.print("\n[bold cyan]ğŸ¤– LLM ë¹Œë” í…ŒìŠ¤íŠ¸ ì‹œì‘[/bold cyan]\n")
    console.print("[yellow]â³ ContextRecallEvaluator ì´ˆê¸°í™” ì¤‘...[/yellow]")
    evaluator = ContextRecallEvaluator()
    console.print("[green]âœ“ ì´ˆê¸°í™” ì™„ë£Œ[/green]\n")
    visualize_build_evaluator_llm_function(evaluator)


@app.command("search")
def cmd_search(
    query: str = typer.Option(
        ...,
        "--query",
        help="ê²€ìƒ‰í•  ì§ˆë¬¸ ë˜ëŠ” ì¿¼ë¦¬",
    ),
    top_k: int = typer.Option(
        5,
        "--top-k",
        help="ê²€ìƒ‰í•  ê²°ê³¼ ê°œìˆ˜",
    ),
):
    """
    _search_contexts() í•¨ìˆ˜ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

    ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ê³¼ì •ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    console.print("\n[bold cyan]ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘[/bold cyan]\n")
    console.print("[yellow]â³ ContextRecallEvaluator ì´ˆê¸°í™” ì¤‘...[/yellow]")
    evaluator = ContextRecallEvaluator()
    console.print("[green]âœ“ ì´ˆê¸°í™” ì™„ë£Œ[/green]\n")
    visualize_search_contexts_function(evaluator, query, top_k)


@app.command("extract")
def cmd_extract(
    query: str = typer.Option(
        ...,
        "--query",
        help="ê²€ìƒ‰ í›„ ì¶”ì¶œí•  ì¿¼ë¦¬",
    ),
    top_k: int = typer.Option(
        5,
        "--top-k",
        help="ê²€ìƒ‰í•  ê²°ê³¼ ê°œìˆ˜",
    ),
):
    """
    _extract_contexts_and_images() í•¨ìˆ˜ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

    ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ IDë¥¼ ì¶”ì¶œí•˜ëŠ” ê³¼ì •ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    console.print("\n[bold cyan]ğŸ“¤ ë°ì´í„° ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹œì‘[/bold cyan]\n")
    console.print("[yellow]â³ ContextRecallEvaluator ì´ˆê¸°í™” ì¤‘...[/yellow]")
    evaluator = ContextRecallEvaluator()
    console.print("[green]âœ“ ì´ˆê¸°í™” ì™„ë£Œ[/green]\n")

    # ë¨¼ì € ê²€ìƒ‰ ìˆ˜í–‰
    search_results = evaluator._search_contexts(query, top_k)
    console.print(f"[green]âœ“ ê²€ìƒ‰ ì™„ë£Œ:[/green] {len(search_results)}ê°œì˜ ê²°ê³¼\n")

    # ì¶”ì¶œ ì‹œê°í™”
    visualize_extract_contexts_and_images_function(evaluator, search_results)


@app.command("recall")
def cmd_recall(
    jsonl: str = typer.Option(
        ...,
        "--jsonl",
        help="í‰ê°€í•  ë°ì´í„°ì…‹ JSONL íŒŒì¼ ê²½ë¡œ",
    ),
    top_k: int = typer.Option(
        5,
        "--top-k",
        help="ê²€ìƒ‰í•  ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜",
    ),
    no_confirm: bool = typer.Option(
        False,
        "--no-confirm",
        help="API í˜¸ì¶œ í™•ì¸ ë©”ì‹œì§€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤",
    ),
):
    """
    _compute_average_recall() í•¨ìˆ˜ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤. (âš ï¸ API ë¹„ìš© ë°œìƒ)

    RAGASë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ë¦¬ì½œì„ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.
    ì‹¤ì œ OpenAI APIë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ ë¹„ìš©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    console.print("\n[bold cyan]ğŸ“Š ë¦¬ì½œ ê³„ì‚° í…ŒìŠ¤íŠ¸ ì‹œì‘[/bold cyan]\n")

    # API í˜¸ì¶œ í™•ì¸
    if not no_confirm:
        console.print(
            "[yellow]âš ï¸ ì´ í…ŒìŠ¤íŠ¸ëŠ” ì‹¤ì œ OpenAI APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë¹„ìš©ì´ ë°œìƒí•©ë‹ˆë‹¤.[/yellow]"
        )
        user_input = typer.prompt("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)")
        if user_input.lower() != "y":
            console.print("[dim]í…ŒìŠ¤íŠ¸ë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.[/dim]")
            raise typer.Exit()

    # ë°ì´í„° ë¡œë“œ
    samples = load_test_data(jsonl)
    if not samples:
        console.print("[red]âŒ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.[/red]")
        raise typer.Exit(code=1)

    # Evaluator ì´ˆê¸°í™”
    console.print("[yellow]â³ ContextRecallEvaluator ì´ˆê¸°í™” ì¤‘...[/yellow]")
    evaluator = ContextRecallEvaluator()
    console.print("[green]âœ“ ì´ˆê¸°í™” ì™„ë£Œ[/green]\n")

    # ì²« ë²ˆì§¸ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸
    test_query = samples[0]["question"]
    search_results = evaluator._search_contexts(test_query, top_k)
    contexts, _ = evaluator._extract_contexts_and_images(search_results)

    # RAGAS ë°ì´í„°ì…‹ êµ¬ì„±
    mock_ragas_dataset = [
        {
            "user_input": test_query,
            "retrieved_contexts": contexts,
            "reference": samples[0].get("ground_truth", "Test reference"),
        }
    ]

    # ë¦¬ì½œ ê³„ì‚°
    visualize_compute_average_recall_function(evaluator, mock_ragas_dataset)


@app.command("pipeline")
def cmd_pipeline(
    jsonl: str = typer.Option(
        ...,
        "--jsonl",
        help="í‰ê°€í•  ë°ì´í„°ì…‹ JSONL íŒŒì¼ ê²½ë¡œ",
    ),
    top_k: int = typer.Option(
        10,
        "--top-k",
        help="ê²€ìƒ‰í•  ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜",
    ),
    no_confirm: bool = typer.Option(
        False,
        "--no-confirm",
        help="API í˜¸ì¶œ í™•ì¸ ë©”ì‹œì§€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤",
    ),
):
    """
    evaluate_context_recall_at_k() ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤. (âš ï¸ API ë¹„ìš© ë°œìƒ)

    ì „ì²´ í‰ê°€ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ì—¬ Context Recall@Kë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    ì‹¤ì œ OpenAI APIë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ ë¹„ìš©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    console.print("\n[bold cyan]ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘[/bold cyan]\n")

    # API í˜¸ì¶œ í™•ì¸
    if not no_confirm:
        console.print(
            "[yellow]âš ï¸ ì´ í…ŒìŠ¤íŠ¸ëŠ” ì‹¤ì œ OpenAI APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë¹„ìš©ì´ ë°œìƒí•©ë‹ˆë‹¤.[/yellow]"
        )
        user_input = typer.prompt("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)")
        if user_input.lower() != "y":
            console.print("[dim]í…ŒìŠ¤íŠ¸ë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.[/dim]")
            raise typer.Exit()

    visualize_full_evaluation_pipeline(jsonl, top_k)


if __name__ == "__main__":
    app()
